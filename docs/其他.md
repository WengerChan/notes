# 其他, 大杂烩


## 小数二进制表示

十进制的小数转换为二进制小数，主要是利用小数部分乘2，取整数部分，直至小数点后为0。

以 0.625 为例：

| 步骤 | 0.625            | 十进制转二进制         |
| ---- | ---------------- | ---------------------- |
| 1    | 0.625 * 2 = 1.25 | 整数部分取 1, 余 0.25  |
| 2    | 0.25 * 2 = 0.5   | 整数部分没有 0, 余 0.5 |
| 3    | 0.5 * 2 = 1.0    | 整数部分取 1, 余 0.0   |

需要注意的是，如果尾数不是 0 或 5（比如 9.624），就无法用一个二进制数来精确表达。因此一些编程语言/数据库中的 浮点类型数据 会有不准确的问题。

```sh
~] echo 'obase=2;0.625' | bc
.1010000000

~] echo 'obase=2;0.624' | bc
.1001111110
```


## `sshpass` 常见用法

```sh
sshpass -p123456 ssh-copy-id -i ~/.ssh/id_rsa.pub " root@172.16.1.$ip  -o StrictHostKeyChecking=no "
sshpass -p123qweQ ssh root@192.168.163.241 'ls /root'
sshpass -f ~/password ssh root@192.168.163.241 'ls /root'
sshpass -p123qweQ scp -o StrictHostKeyChecking=no root@192.168.1.12:/tmp/testfile /root/
sshpass -p123qweQ ssh -o StrictHostKeyChecking=no root@192.168.1.103 "lsblk "
```


## 红帽订阅管理

### 注册

```bash
# 将系统注册到rhn（输入用户名和密码）
subscription-manager register

# 列出系统所有可用的订阅，并记录你在系统激活的订阅池Id
subscription-manager list --available --all

# 使用订阅池id激活订阅
subscription-manager attach --pool=8a85f98154ef8eb40154f1b1d3620670

# 关闭系统所有仓库
subscription-manager repos --disable="*"

# 仅打开系统 server 仓库。
subscription-manager repos --enable=rhel-7-server-rpms

# 列出系统所有仓库。
yum repolist

# 如果需要重新注册，执行以下命令组
sudo subscription-manager remove --all
sudo subscription-manager unregister
sudo subscription-manager clean
sudo subscription-manager register
sudo subscription-manager refresh
sudo subscription-manager attach --auto
```

### 使用

* 用于正常安装软件、补丁等
* 同步补丁，用于搭建补丁源

    ```sh
    # 缓存少量指定包
    yum install ntp --downloadonly --downloaddir=/tmp/

    # 同步整个 repo
    # 7.x 及以下版本
    reposync --plugins --newest-only --delete --download_path=<download_path> --repoid=<repo_id>
    # 8.x 版本
    dnf reposync --newest-only --delete --download-metadata --download-path=<download_path> --repoid=<repo_id>
    ```


### 附: 关于repodata

* `repodata/` 目录下的文件信息

    执行 `createrepo` 会在当前目录下生成一个 `repodata` 的文件夹, 里面一般有以下几个文件:

    - `primary.xml.gz`: 包含所有rpm文件列表、依赖关系、软件包安装列表 
    - `filelists.xml.gz`: 包含所有rpm包的配置文件列表  
    - `other.xml.gz`: 包含软件包其他信息，比如更改记录  
    - `comps.xml`: 包含软件包组的列表，控制软件包 `group` 安装  
    - `repomd.xml`: 包含 `primary`/`filelist`/`other` 文件的时间戳、检验等等之类   

* `yum` 安装 rpm 包执行过程

    - (1) 在 `primary.xml` 里找到需要安装的包  
    - (2) 在 `primary.xml` 中获取到安装包完整名词和依赖包列表  
    - (3) 在 `primary.xml` 中根据 `<location href=xxx/>`获取安装包路径
    - (4) 在 `primary.xml` 中获取依赖包名和对应的 `pkgid`，并在 `filelists.xml` 中获取到配置文件
    - (5) 根据已有信息获取 rpm 包并安装  

* `repodata` 中的 "路径问题"

    首先，有以下三点说明:

    * `yum` 读取 `repodata/primary.xml` 中记录的 `<location href=/>` 时，是以 `repodata` 目录所在那一级目录为基准点的（相对路径），而执行 `createrepo` 命令生成 `repodata` 的所在目录以及 `primary.xml`，是由我们指定的路径决定的。

    * `createrepo` 默认在当前目录下生成 `repodata/` 目录, 可通过 `-o` 指定

    * yum 源 `repo` 文件中配置项 `baseurl` : Must be a URL to the directory where the yum repository's *repodata* directory lives. Can be an ++http://++, ++ftp://++ or ++file://++ URL.

    综上三点可知：

    * 执行 `yum` 安装 `rpm` 包时，获取实际 `rpm` 包的路径为

        ```
        "repo 文件中配置项 baseurl 的路径"/"primary.xml 中 <location href=xxx> 记录的路径"
        ```

        例如: 下面这种情况下，yum 安装 rpm 包时会以 `/media/t/tree-1.6.0-10.el7.x86_64.rpm` 这个路径下载 `tree`
        
        ```
        ~] cat /etc/yum.repos.d/media.repo
        ...
        baseurl=/media/
        ...

        ~] ls /media
        ...
        repodata/
        ...

        ~] cat /media/repodata/xxxxx...primary.xml
        ...
        <location href="t/tree-1.6.0-10.el7.x86_64.rpm">
        ...
        ```

* `createrepo` 的路径问题

    ```sh
    ~] tree /media/
    .
    └── Packages
        └── dir1
            ├── x1.rpm
            ├── ...
            └── xn.rpm
    ```
    
    * 情景一

        ```sh
        ~] cd /media
        ~] createrepo /media/Packages
        ~] tree /media
        .
        ├── Packages
        |   └── dir1
        |       ├── x1.rpm
        |       ├── ...
        |       └── xn.rpm
        └── repodata
            ├── ...primary.xml.gz
            ├── ...
            └── repomd.xml

        ~] gzip -d /media/repodata/primary.xml -c | grep 'location href'
        <location href="dir1/x1.rpm"/>
        <location href="dir1/x2.rpm"/>
        ...
        <location href="dir1/xn.rpm"/>
        ```

    * 情景二

        ```sh
        ~] cd /media
        ~] createrepo /media/
        ~] tree /media
        .
        ├── Packages
        |   └── dir1
        |       ├── x1.rpm
        |       ├── ...
        |       └── xn.rpm
        └── repodata
            ├── ...primary.xml.gz
            ├── ...
            └── repomd.xml

        ~] gzip -d /media/repodata/primary.xml -c | grep 'location href'
        <location href="Packages/dir1/x1.rpm"/>
        <location href="Packages/dir1/x2.rpm"/>
        ...
        <location href="Packages/dir1/xn.rpm"/>
        ```

    以上两种情景，repo 配置文件中 baseurl 都应该配置为 baseurl=file:///mnt，安装 x1.rpm 时，搜索路径为

    ```text
        情景一： /media/dir1/x1.rpm          # 异常
        情景一： /media/Packages/dir1/x1.rpm # 正常
    ```
    
    综上，使用 `createrepo` 的最佳实践为:  

    ```bash
    createrepo /path/of/package -o /path/of/package
    # 或
    createrepo .
    ```

## 单用户模式

### RHEL/Centos 6.x

```text
select "kernel…"

s 或者 single

passwd root

reboot
```
 
### RHEL/Centos 7.x/8.x

使用 `runlevel1.target` 或 `rescue.target` 实现: 

* 在此模式下, 系统会 **挂载所有的本地文件系统**, 但不会开启网络接口。
* 系统仅启动特定的几个服务和修复系统必要的尽可能少的功能
* 常用场景：
    * 修复损坏的文件系统
    * 重置root密码
    * 修复系统上的一个挂载点问题

进入单用户模式的三种方法:

* 方法 1：通过向内核添加 `rd.break` 参数

    * linux16 这一行添加:

        ```sh
        rd.break
        ```

    * `ctrl+x` 引导进入系统, 执行以下命令修改 `/sysroot` 为读写(rw)

        ```sh
        mount -o remount,rw /sysroot/
        ```

    * 切换环境
    
        ```sh
        chroot /sysroot/
        ```

    * 7/8版本系统默认使用 SELinux, 因此创建下面的隐藏文件, 这个文件会在下一次启动时重新标记所有文件
    
        ```sh
        touch /.autorelabel
        reboot
        ```

* 方法 2：通过用 `init=/bin/bash` 或 `init=/bin/sh` 替换内核中的 `rhgb quiet` 语句

    * `init=/bin/bash` 或 `init=/bin/sh` 替换内核中的 `rhgb quiet` 语句

    * 重新挂载 `/`
    
        ```sh
        mount -o remount,rw /
        ```

    * 执行完操作后, 创建标记文件并重启

        ```sh
        touch /.autorelabel
        exec /sbin/init 6
        ```

* 方法 3：通过用 `rw init=/sysroot/bin/sh` 替换内核中的 `ro` 语句

    * `rw init=/sysroot/bin/sh` 替换内核中的 `ro` 单词

    * 切换环境

        ```sh
        chroot /sysroot
        ```

    * 执行完操作后, 创建标记文件并重启

        ```sh
        touch /.autorelabel
        reboot -f
        ```

### SuSE 12

```sh
init=/bin/bash

mount -o remount,rw /

echo 'root:password' | /usr/sbin/chpasswd

mount -o remount,ro /
```

### Ubuntu

高级选项 => recovery模式 => 

修改: 

```sh
ro recovery nomode set ==> rw single init=/bin/bash
```

### Kylin V10

> GRUB密码: root/Kylin123123

```sh
# 华为泰山
init=/bin/bash console=tty1
# 飞腾
init=/bin/bash console=tty0

...

/sbin/reboot -f
```


## 恢复 grub

### 1 从备份文件中恢复

> 适用于主引导分区被破坏,无法通过手动输入引导命令解决

* 1.1 备份

    MBR位于第一块硬盘( `/dev/sda` ) 的第一个物理扇区处, 总共 512 字节, 前 446 字节是主引导记录, 分区表保存在 MBR 扇区中的第 447-510 字节中

    ```sh
    dd if=/dev/vda of=/root/grub.bak bs=446 count=1
    ```

* 1.2 模拟主引导记录丢失

    ```sh
    dd if=/dev/zero of=/dev/vda bs=446 count=1
    ```

此时重启, 服务器卡在 `boot from hard disk`

* 1.3 修复

    挂iso进入救援模式, 然后依次执行以下命令修复

    ```sh
    chroot /mnt/image
    dd if=/root/grub.bak of=/dev/vda bs=446 count=1
    reoot
    ```

### 2 手动输入引导命令

> 适用于主引导分区未被破坏, grub文件配置错误或者丢失 (此时重启系统会提示 `grub>` )

* 2.1 RHEL/CentOS 6

    > 6 系的`grub>`命令有限,  如 `ls` 等命令都没有, 因此只能使用 tab 补全

    - 模拟`grub.cfg`文件丢失, 重启后系统进入 `grub >` 提示符界面

        ```sh
        $ mv /boot/grub/grub.conf /tmp
        $ reboot
        ```

    - 选择磁盘

        ```sh
        grub> root (hd0,0)
        ```

    - 选择内核文件, 同时指定内核参数

        ```sh
        grub> kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/vda2 selinux=1  # 只指定了有限的参数: "根目录挂载", "SELinux状态设置"
        ```

    - 选择`initrd`(初始化时的根文件系统)

        ```sh
        grub> initrd /initramfs-2.6.32-358.el6.x86_64.img
        ```

    - 启动

        ```sh
        grub> boot
        ```

    - 进入系统以后, 还原 `grub.conf` 配置文件, 有以下两种方法: 

        * 1.手动编辑一份
        * 2.从同版本的系统复制一份, 修改参数 (若是统一模板的虚拟机, 参数无需修改)


        > RHEL 6.4 /boot/grub/grub.conf:
        >   ```conf
        >   default=0
        >   timeout=5
        >   splashimage=(hd0,0)/grub/splash.xpm.gz
        >   hiddenmenu
        >   title Red Hat Enterprise Linux (2.6.32-358.el6.x86_64)
        >       root (hd0,0)
        >       kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=UUID=1b0a442f-e911-4c61-8558-bb1f167affde rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=128M  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet console=ttyS0
        >	    initrd /initramfs-2.6.32-358.el6.x86_64.img
        >    ```


* 2.2 RHEL/CentOS 7

    - 模拟`grub.cfg`文件丢失, 重启后系统进入 `grub >` 提示符界面
        
        ```sh
        $ mv /boot/grub2/grub.cfg /tmp/
        $ reboot
        ```

    - 查看
        
        ```sh
        grub> ls
        (hd0) (hd0,msdos3) (hd0,msdos2) (hd0,msdos1)

        grub> ls (hd0,msdos1)
            Partition hd0,msdos1: Filesystem type xfs, UUID xxxxxxxxxxxxxxxxxxx - Partition start at 1024KiB - Total size 1048576Kib <= 这个约1024Mb的就是分配给/boot的分区, 指定这个
        
        grub> cat (hd0,msdos1)/grub2/device.map
        (hd0)   /dev/vda                                   <=hd0 磁盘在系统中名称为vda
        ...
        
        grub> ls (hd0,msdos3)
            Partition hd0,msdos3: Filesystem type xfs, UUID xxxxxxxxxxxxxxxxxxx - Partition start at 2098176KiB - Total size 8387584Kib <= 这个约8G的就是分配给/的分区, linux16中需要root指定(root=/dev/vda3)
        ```
    
    - 选择磁盘
    
        ```sh
        grub > set root=(hd0,msdos1)
        ```
    
    - 选择内核文件, 同时指定内核参数
    
        ```sh
        grub > linux16 /vmlinuz-3.10.0-1160.el7.x86_64 ro root=/dev/vda3

        # 如果使用的是lvm系统, 要指定root的lv路径: 
        # grub > linux16 /vmlinuz-3.10.0-1160.el7.x86_64 ro root=/dev/mapper/rhel-root rd.lvm.lv=rhel/root
        ```
    
    - 选择`initrd`(初始化时的根文件系统)
    
        ```sh
        grub > initrd16 /initramfs-3.10.0-1160.el7.x86_64.img
        ```
    
    - 启动
    
        ```sh
        grub > boot
        ```
    
    - 进入系统以后需要还原 `grub.cfg` 配置文件
    
        ```sh
        grub2-mkconfig -o /boot/grub2/grub.cfg
        ```


### 3 救援模式下重新生成 grub 文件

> 此方法需要使用 `grub2-install`/`grub-install` 命令

* RHEL/CentOS 6

    ```text
    sh-4.1# grub-install /dev/vda
    sh-4.1# vi /boot/grub/grub.conf

    default=0
    timeout=5
    title RHEL 6.4
    kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/vda2 selinux=1
    initrd /initramfs-2.6.32-358.el6.x86_64.img
    ```

* RHEL/CentOS 7

    ```text
    sh-4.2# grub2-install /dev/vda
    sh-4.2# grub2-mkconfig -o /boot/grub2/grub.cfg
    ```


## 系统启动过程

### CentOS6

![Linux启动过程-Centos6](./pictures/其他/Linux启动过程-Centos6.png)

* 1. 开机自检

    对硬件进行检查，正确检查以后进入下一步

* 2. 读取MBR

    **主引导记录**，共512bytes(**0柱面、0磁道、1扇区前512字节**)

    - 前446字节为 **引导记录区**，记录了哪些活动分区，用于找到活动的分区，并将活动分区的引导记录写入内存  
    - 后66字节记录 **磁盘的分区信息**，前 64字节 记录磁盘分区表信息，后2字节是分区的结束标志  

* 3. 加载GRUB菜单

    `bootloader` 被运行以后，读取 `/etc/grub.conf` ( `/boot/grub/menu.lst`, `/boot/grub/grub.conf` )

    ```text
    # grub.conf generated by anaconda
    #
    # Note that you do not have to rerun grub after making changes to this file
    # NOTICE:  You have a /boot partition.  This means that
    #          all kernel and initrd paths are relative to /boot/, eg.
    #          root (hd0,0)
    #          kernel /vmlinuz-version ro root=/dev/mapper/vg_rheltest6-lv_root
    #          initrd /initrd-[generic-]version.img
    #boot=/dev/sda
    default=0                                  #默认情况下如何加载系统，0表示加载菜单中对应的第一个名字
    timeout=5                                  #表示多少秒后开始加载默认系统
    splashimage=(hd0,0)/grub/splash.xpm.gz     #启动时显示的背景图片，(hd0,0)表示/boot分区
    hiddenmenu                                 #启动时默认会隐藏菜单信息，按默认设置启动系统；除非用户按键干预
    #password [-md5|-encrypted] STRING         #此项默认没有。编辑grub菜单需要认证
    title Red Hat Enterprise Linux Server (2.6.32-754.15.3.el6.x86_64)    #系统对应的名称，可按需修改
            root (hd0,0)                                                  #引导内核文件和内核所需驱动文件所在的分区，(hd0,0)表示/boot分区。hd0表示计算机的第一块磁盘，逗号后面的0表示第一个分区。
            kernel /vmlinuz-2.6.32-754.15.3.el6.x86_64 ro root=/dev/mapper/vg_rheltest6-lv_root rd_LVM_LV=vg_rheltest6/lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=vg_rheltest6/lv_swap rd_NO_DM rhgb quiet
                                                                          #位于boot分区的内核文件，以及一些可选内核参数
                                                                          #/vmlinuz-2.6.32-754.15.3.el6.x86_64为内核文件(/boot下)；root=/dev/mapper/vg_rheltest6-lv_root 表示根对应的设备信息(有时候也写成UUID)
            initrd /initramfs-2.6.32-754.15.3.el6.x86_64.img              #临时的初始根文件系统，init程序也在其中；内核启动所需的驱动文件，存在于boot区
    
    
    # Linux内核在初始化之后会执行init进程，而init进程会挂载我们的根文件系统，但由于init程序也是在根文件系统上的，所以这就有了悖论。
    # 2.6以前 initrd.img，2.6以后initramfs.img ；
    # initramfs 的工作方式：系统启动的时候加载内核和 initramfs 到内存执行，内核初始化之后，切换到用户态执行 initramfs 的程序/脚本，加载需要的驱动模块、必要配置等，然后加载 rootfs 切换到真正的 rootfs 上去执行后续的 init 过程
    ```
* 4. 加载内核

    根据 `GRUB` 设定的路径读取内核映像，运行解压缩，解压完成后输出 ***OK，booting the kernel*** 信息。

    其实是根据 `grub.conf` 中的 `kernel /vmlinuz-2.6.32-754.15.3.el6.x86_64 ro root=/dev/mapper/vg_rheltest6-lv_root xxxx` 的设定 **加载内核及相关参数**，同时还会**加载内核所需要的驱动程序文件**(`initrd /initramfs-xxxxxxx.img`)，进而挂载并读取根分区的信息，加载操作系统

* 5. 启动 `init` 进程

    内核初始化以后，`/sbin/init`进程首先启动，其PID为1

* 6. `init` 进程读取 `/etc/inittab` ，设定运行级别

    init 启动以后立刻读取 `/etc/inittab` 设定的系统的运行级别: `id:3:initdefault:`，从而影响后面运行哪部分的文件

* 7. 初始化系统、加载内核模块

    * 7.1 `init` 进程读取 `/etc/init/rcS.conf`，初始化系统

        关于`/etc/rc.d/rc.sysinit`:

        - CentOS6以前，init进程根据 `/etc/inittab` 中的设置加载 `/etc/rc.d/rc.sysinit`，进行初始化。系统设置包括但不限于：**设置主机名**、**设置欢迎信息**、**激活udev和selinux**、**加载`/etc/fstab`并挂载**、**设置时间**、**读取`/etc/sysctl.conf`设置内核参数**、**激活LVM及software raid设备**、**加载额外设备的驱动程序**、**各种清理动作**（如清理日志）等

        - CentOS6中，init进程不再通过读取 `/etc/inittab` 而加载 `/etc/rc.d/rc.sysinit`，而是读取 `/etc/init/rcS.conf` 文件加载 `/etc/rc.d/rc.sysinit`，然后对系统进行初始化系统设置

    * 7.2 `init` 进程加载内核相关模块

        CentOS6以前，init进程读取 `/etc/modules.conf` 文件或 `/etc/modules.d` 目录下的文件加载内核模块(centos6中已经没有这两个)

        CentOS6，加载 `/etc/sysconfig/modules/` 下的内核模块

* 8. `init` 进程运行对应运行级别的开机脚本

    运行以下目录下的脚本：

    ```sh
    /etc/rc0.d/ ~ /etc/rc6.d   # 指向/etc/rc.d/rc0.d/ -- /etc/rc.d/rc6.d
    ```

    CentOS6，`init` 进程不再通过读取 `/etc/inittab` 加载运行级别对应的脚本了，而是读取 `/etc/init/rc.conf` 加载指定运行级别对应目录的脚本:
	
    - centos 5.5 通过 `/etc/inittab` 调起 `/etc/rc.d/rc`，进而执行 `/etc/rcN.d/*`

        ![Picture](pictures/其他/Linux启动过程-Centos5.5_inittab.png)

    - centos 6中，`/etc/init/rc.conf` 最后一行会调起 `/etc/rc.d/rc $RUNLEVEL`。由 `/etc/rc.d/rc` 执行一部分设置，并调起 `/etc/rcN.d/` 下的脚本

        ![Picture](pictures/其他/Linux启动过程-Centos6_rc.conf.png)


* 9. 加载 `/etc/rc.local`

    ```sh
    /etc/rc.local # 指向/etc/rc.d/rc.local
    ```

    系统做好一切初始化工作以后，开始自动执行 `/etc/rc.local` 文件记录的开机执行命令

* 10. 启动 `mingetty`，进入登录前状态

    系统读取 `/etc/init/tty.conf` (早期也是读取 `/etc/inittab` 进行设置的)

    ![Picture](pictures/其他/Linux启动过程-mingetty.png)


### CentOS7

![Linux启动过程-Centos7](./pictures/其他/Linux启动过程-Centos7.png)

* 1. 启动进程变化

    CentOS7 和 CentOS6 的启动流程绝大部分还是相同的，但也有一些小区别。例如，CentOS6 下第一个启动的 `init` 进程被改为了 `systemd`（并行启动模式）

    **CentOS7 是并行启动，各个unit之间并无明显的先后顺序**

* 2. 文件的变化

    名称: grub => grub2

    | centos6              | centos7              |
    | -------------------- | -------------------- |
    | /etc/grub.conf       | /etc/grub2.cfg       |
    | /boot/grub/menu.lst  | /boot/grub2/grub.cfg |
    | /boot/grub/grub.conf |                      |
    |                      | /etc/default/grub    |
    |                      | /etc/grub.d/         |

* 3. grub2 启动引导阶段

    | 文件       | 作用                               |
    | ---------- | ---------------------------------- |
    | `boot.img` | 唯一作用是在系统启动时装载core.img |
    | `core.img` | grub2的核心映像文件                |
    | `*.mod`    | 可动态加载的模块                   |
    | `grub.cfg` | 核心配置文件                       |

* 4. 加载内核和initramfs

    从磁盘加载到ram。

    `initramfs`是cpio的归档文件，**包含必要的kernel模块以及初始化脚本等**

* 5. 启动systemd进程

    从initramfs启动systemd的工作副本（`/sbin/systemd`, pid=0）

* 6. 加载 `initrd.target` (`/usr/lib/systemd/system/initrd.target`)所有单元，包括 `/etc/fstab`

    完成以后，控制权会移交给root文件系统的systemd实例

* 7. 加载 `default.target`，设定运行级别

* 8. 加载sysinit.target，初始化系统及加载basic.target

* 9. 启动对应运行级别(如multi-user.target)下的服务程序

    ```sh
    /etc/systemd/system/
    /usr/lib/systemd/system/
    ```

* 10. 读取并执行/etc/rc.d/rc.local

* 11. 加载getty.target

* 12. 启动graphical所需服务（图形桌面）


### 附: systemd

```sh
man bootup           # systemd启动
man systemd.special  # 查看各个systemd的unit含义
```

**SYSTEM MANAGER BOOTUP**

&emsp;The following chart is a structural overview of these well-known units and their position in the boot-up logic. The arrows describe which units are pulled in and ordered before which other units. Units near the top are started before units nearer to the bottom of the chart.

![Linux启动过程-Centos7_bootup](pictures/其他/Linux启动过程-Centos7_bootup_01.png)

**BOOTUP IN THE INITIAL RAM DISK (INITRD)**

&emsp;The default target in the initrd is initrd.target. The bootup process begins identical to the system manager bootup (see above) until it reaches basic.target. From there, systemd approaches the special target initrd.target.   

&emsp;If the root device can be mounted at /sysroot, the sysroot.mount unit becomes active and initrd-root-fs.target is reached.   
&emsp;The service initrd-parse-etc.service scans /sysroot/etc/fstab for a possible /usr mount point and additional entries marked with the x-initrd.mount option. All entries found are mounted below /sysroot, and initrd-fs.target is reached.   
&emsp;The service initrd-cleanup.service isolates to the initrd-switch-root.target, where cleanup services can run. As the very last step, the initrd-switch-root.service is activated, which will cause the system to switch its root to /sysroot.

![Linux启动过程-Centos7_bootup](pictures/其他/Linux启动过程-Centos7_bootup_02.png)

**SYSTEM MANAGER SHUTDOWN**

&emsp;System shutdown with systemd also consists of various target units with some minimal ordering structure applied

![Linux启动过程-Centos7_bootup](pictures/其他/Linux启动过程-Centos7_bootup_03.png)


## 自定义网卡名

### 1 RHEL/CentOS 6

目的及预期：当前使用的是 `eth0`, MAC地址 `00:0c:29:4c:1b:f1` 。通过将 `eth0` 和 `eth1` 网卡名互换, 由 MAC 地址为 `00:0c:29:4c:1b:fb` 的网卡提供服务, 并保证网卡名依旧为 `eth0` 。

![Picture](./pictures/Linux/Linux-自定义网卡名-01.png)

* 1.1 查看网卡驱动

    通过ethtool查看网卡的驱动：检查driver字段的值, 此处为e1000

    ```bash
    ethtool -i eth0
    ethtool -i eth1
    ```

    ![Picture](./pictures/Linux/Linux-自定义网卡名-02.png)


* 1.2 卸载网卡驱动

    通过 `modprobe` 卸载驱动

    ```bash
    modprobe -r e1000
    ```

    ![Picture](./pictures/Linux/Linux-自定义网卡名-03.png)


* 1.3 修改配置文件

    修改 `/etc/udev/rules.d/70-persistent-net.rules` 文件, 修改 `Name="eth0"` 和 `Name="eth1"` 处:

    ```bash
    cp /etc/udev/rules.d/70-persistent-net.rules /root/70-persistent-net.rules.bak      # 备份
    vi /etc/udev/rules.d/70-persistent-net.rules
    ```

    ![Picture](./pictures/Linux/Linux-自定义网卡名-04.png)


* 1.4 将之前卸载的网卡驱动重新加载

    ```bash
    modprobe e1000
    ```

* 1.5 验证

    可以看见, 网卡名已经修改, 由 MAC 地址为 `00:0c:29:4c:1b:fb` 的网卡提供服务

    ![Picture](./pictures/Linux/Linux-自定义网卡名-05.png)


### 2 RHEL/CentOS 7

RHEL 7 中, systemd 采用新的命名规则来命名网卡, rename 步骤：

* Step 1

    `/usr/lib/udev/rules.d/60-net.rules` 文件中的规则会让 `udev helper utility` (即 `/lib/udev/rename_device` ) 检查所有 `/etc/sysconfig/network-scripts/ifcfg-suffix` 文件。  

    如果发现包含 `HWADDR` 条目的 `ifcfg-文件` 与某个接口的 MAC 地址匹配, 它会将该接口重命名为 `ifcfg-文件` 中由 `DEVICE` 关键字配置的名称。

    > 注: DEVICE设置的名称应该与文件名相对应。即在配置文件 `ifcfg-suffix` 中, 应该保证 `DEVICE=suffix`

* Step 2

    `/usr/lib/udev/rules.d/71-biosdevname.rules`

    对于 *Step 1* 中没有被 rename 的接口, 当安装了 `biosdevname`, 并且内核参数未设置 `biosdevname=0` 时, `/usr/lib/udev/rules.d/71-biosdevname.rules`<sup>由 `biosdevname` 提供的文件</sup> 中的规则会让 `biosdevname` 根据其命名策略重命名该接口。

* Step 3

    `/lib/udev/rules.d/75-net-description.rules` 中的规则让 udev 通过检查网络接口设备, 填写内部 udev 设备属性值 `ID_NET_NAME_ONBOARD`、`ID_NET_NAME_SLOT`、`ID_NET_NAME_PATH`。

    注：有些设备属性可能处于未定义状态undefined

* Step 4

    `/usr/lib/udev/rules.d/80-net-name-slot.rules` 中的规则让 udev 重命名 *Step1* 和 *Step2* 中未命名的网卡。

    内核参数未设置 `net.ifname=0` 时, 按照文件中设置的规则顺序命名。如果这些属性值未设置, 则网卡无法被rename。

    默认的规则顺序如下：

    Naming Schemes Hierarchy: (命名方式层级, 可在 `/usr/lib/udev/rules.d/80-net-name-slot.rules` 设置)

    * `ID_NET_NAME_ONBOARD`: BIOS provided index numbers for on-board devices (example: eno1)  
    * `ID_NET_NAME_SLOT`: BIOS provided PCI Express hotplug slot index numbers (example: ens1)  
    * `ID_NET_NAME_PATH`: physical location of the connector of the hardware (example: enp2s0)  
    * `ID_NET_NAME_MAC`: 整个MAC地址的命名方式 (example: enx000c29050577)  
    * 传统的不可预测的内核命名方案: 其他都失效以后 (example: eth0)  

那么, 当我们要自定义自己的网卡名时, 最直接的办法是编辑 `/etc/sysconfig/network-scripts/ifcfg-suffix` 文件, 将 `suffix`、`DEVICE`、`MAC` 相应改成需要的值

例如, 想要一个名称为 `ensxxxxxxx1` 的网卡, 对应的MAC地址为 `00:0c:29:05:05:81`:

![Picture](./pictures/Linux/Linux-自定义网卡名-06.png)

重启完成设定:

![Picture](./pictures/Linux/Linux-自定义网卡名-07.png)


一般的, 会出现以下几种网卡名(`eth0`, `ens33`, `eno000009`), 可以通过 `/etc/default/grub` 或 `/etc/sysconfig/grub` 中修改 `GRUB_CMDLINE_LINUX` 行修改, 重启后生效, 其对应关系如下：

```text
net.ifnames=0 biosdevname=0 ==> eth0  
net.ifnames=1 biosdevname=0 ==> ens192  
```


## 配置 VLAN

* 通过 nmcli 配置

    ```sh
    nmcli connection add type vlan con-name VLAN99 dev ens224 id 99 ipv4.addresses 10.2.99.102/24 ipv4.method manual
    ```

* 通过配置文件配置 (待验证)

    ```sh
    # ifcfg-ens224.10
    TYPE=Vlan
    VLAN=yes
    VLAN_ID=10
    PHYSDEV=ens224
    NAME=ens224.10
    DEVICE=ens224.10
    BOOTPROTO=none
    IPADDR=192.168.10.1
    NETMASK=255.255.255.0
    #GATEWAY=
    ONBOOT=yes
    DEFROUTE=yes
    ```

查看当前 vlan:

```sh
~] ls /proc/net/vlan/
config  ens224.305  ens224.99

~] cat /proc/net/vlan/config 
VLAN Dev name    | VLAN ID
Name-Type: VLAN_NAME_TYPE_RAW_PLUS_VID_NO_PAD
ens224.305     | 305  | ens224
ens224.99      | 99   | ens224
```


## 磁盘 UUID

```sh
blkid
lsblk -o name,mountpoint,size,uuid
ls -lh /dev/disk/by-uuid/
hwinfo --block | grep by-uuid | awk '{print $3,$7}'
udevadm info -q all -n /dev/sdc1 | grep -i by-uuid | head -1
tune2fs -l /dev/sdc1 | grep UUID
dumpe2fs /dev/sdc1 | grep UUID
```

* VMware 虚拟机磁盘序列号: 

    默认情况下，VMware创建的虚拟机，无法查询到虚拟磁盘的序列号。如果是因为需要应用系统要求或者其他原因，需要系统内显示磁盘序列号，按以下步骤操作。

    ```sh
    # 1. 关闭虚拟机

    # 2. 编辑虚拟机vmx文件，添加以下内容：
    disk.EnableUUID="TRUE"

    # 3. 重启后即可查询

    smartctl -a /dev/sda
    lsblk -d -n -o serial /dev/sda
    ```


## 查看文件创建时间

> 仅支持 `ext4` 文件系统; `ext2`, `ext3`, `xfs` 等不支持

* 获取到文件所在磁盘

    ```sh
    ~] df -hT
    Filesystem    Type    Size  Used Avail Use% Mounted on
    /dev/mapper/vg_01-Log01        # <= 
                ext4     35G   13G   21G  39% /
    tmpfs        tmpfs    939M  491M  449M  53% /dev/shm
    /dev/vda1     ext4    485M   32M  428M   7% /boot
    ```

* 获取到文件的inode号, 使用 `ls -i` 也可查询

    ```sh
    ~] stat /etc/hosts
    File: '/etc/hosts'
    Size: 186             Blocks: 8          IO Block: 4096   regular file
    Device: fd00h/64768d    Inode: 1576689     Links: 1      # <= 
    Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
    Access: 2022-02-10 00:45:20.769000091 +0800
    Modify: 2022-02-10 00:45:19.983000112 +0800
    Change: 2022-02-10 00:45:19.983000112 +0800
    ```

* 查询文件创建时间

    ```sh
    ~] debugfs -R 'stat <1576689>' /dev/mapper/vg_01-Log01    # <= 
    debugfs 1.41.12 (17-May-2010)
    Inode: 1576689   Type: regular    Mode:  0644   Flags: 0x80000
    Generation: 1875514122    Version: 0x00000000:00000001
    User:     0   Group:     0   Size: 186
    File ACL: 0    Directory ACL: 0
    Links: 1   Blockcount: 8
    Fragment:  Address: 0    Number: 0    Size: 0
    ctime: 0x6203ef9f:ea5d90c0 -- Thu Feb 10 00:45:19 2022
    atime: 0x6203efa0:b7580a6c -- Thu Feb 10 00:45:20 2022
    mtime: 0x6203ef9f:ea5d90c0 -- Thu Feb 10 00:45:19 2022
    crtime: 0x61ef1178:80821818 -- Tue Jan 25 04:52:08 2022   # <= 获取到文件创建时间 crtime
    Size of extra inode fields: 28
    Extended attributes stored in inode body: 
    selinux = "system_u:object_r:net_conf_t:s0\000" (32)
    EXTENTS:
    (0): 4756475
    ```

## 磁盘性能测试

磁盘性能指标:

* IOPS: *每秒读/写次数*, 单位为 *次* (计数)。存储设备的底层驱动类型决定了不同的 IOPS。
* 吞吐量: *每秒的读写数据量*, 单位为 `MB/s`。
* 时延: I/O 操作的 *发送时间到接收确认* 所经过的时间, 单位为 *秒*。

### `fio` 工具

| 参数名              | 说明                                                                                                                                        | 样例     |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| `numjobs`           | 测试进程数量                                                                                                                                | 16       |
| `bs`                | 每次请求的块大小: 4k, 8k, 16k等                                                                                                             | 4k       |
| `ioengine`          | I/O 引擎: Linux原生异步I/O引擎                                                                                                              | libaio   |
| `iodepth`           | 请求的 I/O 队列深度                                                                                                                         | 32       |
| `direct`            | 指定 direct 模式: <br>True(1): 表示指定 O_DIRECT 标识符, 忽略 I/O 缓存, 数据直写(默认)<br>False(0): 表示不指定 O_DIRECT 标识符              | 1        |
| `rw`                | 指定读写模式: <br>顺序读(read), 顺序写(write), 随机读(randread), 随机写(randwrite), <br>混合随机读写(randrw)和混合顺序读写(rw, readwrite)。 | read     |
| `time_based`        | 指定采用时间模式。无需设置该参数值, 只要 FIO 基于时间来运行。                                                                               | N/A      |
| `runtime`           | 指定测试时长, 即 FIO 运行时长.                                                                                                              | 300      |
| `refill_buffers`    | FIO 将在每次提交时重新填充 I/O 缓冲区。默认设置是仅在初始时填充并重用该数据                                                                 | N/A      |
| `norandommap`       | 在进行随机 I/O 时, FIO 将覆盖文件的每个块。若给出此参数, 则将选择新的偏移量而不查看 I/O 历史记录                                            | N/A      |
| `randrepeat`        | 随机序列是否可重复, True(1)表示随机序列可重复, False(0)表示随机序列不可重复。默认为 True(1)                                                 | 0        |
| `group_reporting`   | 多个 job 并发时, 打印整个 group 的统计值                                                                                                    | N/A      |
| `name`              | job 的名称                                                                                                                                  | fio-read |
| `size`              | I/O 测试的寻址空: 不指定的时候, 会全盘测试; 如果是文件测试, 不指定大小则会报错                                                              | 100G     |
| `filename`          | 测试对象, 即待测试的磁盘设备名称。                                                                                                          | /dev/sdb |
| `thread`            | 默认使用fork创建job, 指定thread后使用`pthread_create`创建(POSIX Threads' function)                                                          |          |
| `percentage_random` | 指定随机读/写的占比                                                                                                                         | 100,0    |


* 裸盘 IOPS 测试

    * 随机写 IOPS

        ```sh
        fio -numjobs=16 -bs=4k -ioengine=libaio -iodepth=32 -direct=1 -rw=randwrite -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-randwrite-iops -filename=/dev/vdb
        ```

    * 随机读 IOPS

        ```sh
        fio -numjobs=16 -bs=4k -ioengine=libaio -iodepth=32 -direct=1 -rw=randread -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-randread-iops -filename=/dev/vdb
        ```

* 裸盘带宽测试 (顺序读, 写)

    * 顺序写带宽

        ```sh
        fio --numjobs=16 -bs=128k -ioengine=libaio -iodepth=32 -direct=1 -rw=write -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-write-throughput -filename=/dev/vdb
        ```

    * 顺序读带宽

        ```sh
        fio --numjobs=16 -bs=128k -ioengine=libaio -iodepth=32 -direct=1 -rw=read -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-read-throughput -filename=/dev/vdb
        ```

* 裸盘延迟

    * 随机写延迟

        ```sh
        fio --numjobs=16 -bs=4k -ioengine=libaio -iodepth=1 -direct=1 -rw=randwrite -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-randwrite-lat -filename=/dev/vdb
        ```

    * 随机读延迟 

        ```sh
        fio --numjobs=16 -bs=4k -ioengine=libaio -iodepth=1 -direct=1 -rw=randread -time_based -runtime=300  -refill_buffers -norandommap -randrepeat=0 -group_reporting -name=fio-randread-lat -filename=/dev/vdb
        ```

* 非裸盘测试 - 指定文件大小和文件名

    ```sh
    -size=10G -filename=/file/path
    ```

* 关于 `iodepth`

    ![iodepth](./pictures/linux/linux-iodepth.jpeg)

    AHCI 与 NVMe 两种接口之间的对比表明: 

    * AHCI接口只拥有一个队列, 每个队列深度最大为32;
    * NVMe接口最高拥有64K个队列, 每个队列深度最大为64K;

    所以测试 SATA/SAS 接口的硬盘性能时, 队列深度建议最大设为 32, 再高只会降低测试的性能值; 若使用NVMe硬盘, 深度可以适当的增加, 这取决于硬盘本身的性能。

    ```sh
    fio -ioengine=psync -bs=32k -fdatasync=1 -thread -rw=randrw  -size=10G -filename=fio_randread_write_test.txt -name='fio mixed randread and sequential write test' -iodepth=4 -runtime=60 -numjobs=4 -group_reporting 
    ```

### `dd` 工具

* 写

    ```sh
    dd if=/dev/zero of=/mnt/test/testfile bs=4k count=26214400 oflag=direct # 100G
    ```

* 读

    ```sh
    dd if=/root/testfile of=/dev/null bs=4k count=26214400 iflag=direct   # 100G
    ```

* 附: 详解 `dd`

    * 参数

        ```text
        if=文件名 - 输入文件名，缺省为标准输入。即指定源文件。<if=inputfile>
        of=文件名 - 输出文件名，缺省为标准输出。即指定目的文件。< of=output file >
        ibs=bytes - 一次读入bytes个字节，即指定一个块大小为bytes个字节。
        obs=bytes - 一次输出bytes个字节，即指定一个块大小为bytes个字节。
        bs=bytes  - 同时设置读入/输出的块大小为bytes个字节。
        cbs=bytes - 一次转换bytes个字节，即指定转换缓冲区大小。
        skip=blocks - 从输入文件开头跳过blocks个块后再开始复制。
        seek=blocks - 从输出文件开头跳过blocks个块后再开始复制。注意: 通常只用当输出文件是磁盘或磁带时才有效，即备份到磁盘或磁带时才有效。
        count=blocks - 仅拷贝blocks个块，块大小等于ibs指定的字节数。
        conv=<CONVS> - 进行指定格式的操作
            ascii: ebcdic -> ascii
            ebcdic: ascii -> ebcdic
            ibm: ascii -> alternateebcdic
            block: 把每一行转换为长度为 cbs，不足部分用空格填充
            unblock: 使每一行的长度都为cbs，不足部分用空格填充
            lcase: 大写 -> 小写
            ucase: 小写 -> 大写
            swab: 交换输入的每对字节
            noerror: 出错时不停止
            notrunc: 不截断输出文件
            nocreate: 不创建输出文件
            sync: 将每个输入块填充到 ibs 个字节，不足部分用空（NUL）字符补齐。
        iflag=FLAGS
        oflag=FLAGS - 指定读/写的方式标签
	        append: 追加的方式(只对output产生影响)，建议和 conv=notrunc 搭配使用
	        direct: 读写数据采用I/O
	        directory: 非directory就会读写失败
	        dsync: 书写数据采用synchronized I/O
	        sync: 同上，但包括metadata
	        fullblock: 占满b lock (iflag only)
	        nonblack: 使用 non-blocking I/O
	        noatime: 不更新 access time
	        nocache: discard cached data
	        nofollow: do not follow symlinks
	        skip_bytes/counts_bytes (iflag only): 相应的skip=N/count=N变为N bytes 
	        seek_bytes (oflay only): 相应的seek=N变为N bytes
        status=LEVEL: 控制 dd 程序的输出信息
            none: 除error外，不输出任何信息
            noxfer: 不输出最后的统计信息
            progress: 输出所有信息
        ```

    * 常见示例

        ```sh
        # 1.将本地的/dev/hdb整盘备份到/dev/hdd
        dd if=/dev/hdb of=/dev/hdd

        # 2.将/dev/hdb全盘数据备份到指定路径的image文件
        dd if=/dev/hdb of=/root/image

        # 3.将备份文件恢复到指定盘
        dd if=/root/image of=/dev/hdb

        # 4.备份 /dev/hdb 全盘数据，并利用 gzip 工具进行压缩，保存到指定路径
        dd if=/dev/hdb | gzip > /root/image.gz

        # 5.将压缩的备份文件恢复到指定盘
        gzip -dc /root/image.gz | dd of=/dev/hdb

        # 6.备份磁盘开始的512个字节大小的MBR信息到指定文件
        dd if=/dev/hda of=/root/image count=1 bs=512

        # 恢复 MBR
        dd if=/root/image of=/dev/hda

        # 7.备份软盘
        dd if=/dev/fd0 of=disk.img count=1 bs=1440k

        # 8.拷贝内存内容到硬盘
        dd if=/dev/mem of=/root/mem.bin bs=1024

        # 9.拷贝光盘内容到指定文件夹，并保存为cd.iso文件
        dd if=/dev/cdrom of=/root/cd.iso

        # 10.增加swap分区文件大小
        dd if=/dev/zero of=/swapfile bs=1024 count=262144
        mkswap /swapfile
        swapon /swapfile
        # vi /etc/fstab
        # /swapfile swap swap defaults 0 0

        # 11.销毁磁盘数据: 利用随机的数据填充硬盘
        dd if=/dev/urandom of=/dev/hda1

        # 12.测试硬盘的读写速度
        # 通过以下两个命令输出的命令执行时间，可以计算出硬盘的读、写速度
        dd if=/dev/zero bs=1024 count=1000000 of=/root/1Gb.file
        dd if=/root/1Gb.file bs=64k | dd of=/dev/null

        # 13.确定硬盘的最佳块大小
        # 通过比较以下命令输出中所显示的命令执行时间，即可确定系统最佳的块大小
        dd if=/dev/zero bs=1024 count=1000000 of=/root/1Gb.file
        dd if=/dev/zero bs=2048 count=500000 of=/root/1Gb.file
        dd if=/dev/zero bs=4096 count=250000 of=/root/1Gb.file
        dd if=/dev/zero bs=8192 count=125000 of=/root/1Gb.file

        # 14.修复硬盘
        # 当硬盘较长时间（比如1，2年）放置不使用后，磁盘上会产生magnetic fluxpoint。当磁头读到这些区域时会遇到困难，并可能导致I/O错误。当这种情况影响到硬盘的第一个扇区时，可能导致硬盘报废。上边的命令有可能使这些数据起死回生。且这个过程是安全，高效的。
        dd if=/dev/sda of=/dev/sda

        # 15.dd命令做usb启动盘
        # (1) root用户或者sudo  
        # (2) 用以上命令前必须卸载 u 盘, sdb 是u盘
        # (3) 注意, 执行命令后很快完成, 但u盘还在闪, 等不闪了安全移除。
        dd if=xxx.iso of=/dev/sdb bs=1M

        # 16. 
        ## 1) 生成文件大小和实际占空间大小一样的文件
        dd if=/dev/zero of=name.file bs=1M count=1

        ## 2)生成文件大小固定，但实际不占空间命令
        # seek=1000 表示略过1000个Block不写 (这里 Block 按照 bs 的定义是 1M)，count=0 表示写入 0 个Block。
        # 用ls(查看文件大小)命令看新生成的文件，大小可以看出是1000M。但是再用 du 实际占用硬盘大小只有0M 。
        dd if=/dev/zero of=1G.img bs=1M seek=1000 count=0

        # 17. 占用内存
        mkdir /tmp/memory  
        mount -t tmpfs -o size=1024M tmpfs /tmp/memory
        dd if=/dev/zero of=/tmp/memory/block

        sleep 3600
        rm /tmp/memory/block
        umount /tmp/memory
        rmdir /tmp/memory
        ```


## Netem - 网络流量控制 (待验证)

### 概念

Netem 是 Linux 2.6 及以上内核版本提供的一个网络模拟功能模块。该功能模块可以用来在性能良好的局域网中，模拟出复杂的互联网传输性能，诸如低带宽、传输延迟、丢包等等情况。使用 Linux 2.6 (或以上) 版本内核的很多发行版 Linux 都开启了该内核功能，比如Fedora、Ubuntu、Redhat、OpenSuse、CentOS、Debian等等。tc 是 Linux 系统中的一个工具，全名为traffic control（流量控制）。tc 可以用来控制 netem 的工作模式，也就是说，如果想使用 netem ，需要至少两个条件，一个是内核中的 netem 功能被包含，另一个是要有 tc 。

```bash
SYNOPSIS
tc [ OPTIONS ] qdisc  [ add | change | replace | link | delete ] dev DEV [ parent qdisc-id | root ] [ handle qdisc-id ] qdisc [ qdisc specific parameters ]
tc [ OPTIONS ] class  [ add | change | replace | delete ] dev DEV parent qdisc-id [ classid class-id ] qdisc [ qdisc specific parameters ]
tc [ OPTIONS ] filter [ add | change | replace | delete ] dev DEV [ parent qdisc-id | root ] protocol protocol prio priority filtertype [ filtertype specific parameters ] flowid flow-id
tc [ OPTIONS ] [ FORMAT ] qdisc show [ dev DEV ]
tc [ OPTIONS ] [ FORMAT ] class show dev DEV
tc [ OPTIONS ] filter show dev DEV

OPTIONS := { [ -force ] [ -OK ] -b[atch] [ filename ] | [ -n[etns] name ] | [ -nm | -nam[es] ] | [ { -cf | -c[onf] } [ filename ] ] }
FORMAT := { -s[tatistics] | -d[etails] | -r[aw] | -p[retty] | -i[ec] | -g[raph]}
```

![Picture](./pictures/其他/网络流量控制.png)

### 原理

`tc` 用于Linux内核的流量控制，主要是通过在输出端口处建立一个队列来实现流量控制。

接收包从输入接口（Input Interface）进来后，经过流量限制（Ingress Policing）丢弃不符合规定的数据包，再由输入多路分配器（Input De-Multiplexing）进行判断选择：
如果接收包的目的是本主机，那么将该包送给上层处理；否则需要进行转发，将接收包交到转发块（Forwarding Block）处理。

转发块同时也接收本主机上层（TCP、UDP等）产生的包。转发块通过查看路由表，决定所处理包的下一跳。然后，对包进行排列以便将它们传送到输出接口（Output Interface）。一般我们只能限制网卡发送的数据包，不能限制网卡接收的数据包，所以我们可以通过改变发送次序来控制传输速率。Linux流量控制主要是在输出接口排列时进行处理和实现的。

### 应用

可完成如下功能：（故障模拟）模拟时延，丢包，重复包，乱序。

* 1、模拟延迟传输

    **将 eth0 网卡的传输设置为延迟100毫秒发送**

    ```sh
    tc qdisc add dev eth0 root netem delay 100ms
    ```

    更真实的情况下，延迟值不会这么精确，会有一定的波动：**将eth0网卡的传输设置为延迟 100±10 ms(90~110ms之间的任意值)发送**

    ```sh
    tc qdisc add dev eth0 root netem delay 100ms 10ms
    ```

    更进一步加强这种波动的随机性：**将eth0网卡的传输设置为100ms ，同时大约有30%的包会延迟±10ms发送**

    ```sh
    tc qdisc add dev eth0 root netem delay 100ms 10ms 30%
    ```


* 2、模拟网络丢包

    **将eth0网卡的传输设置为随机丢掉1%的数据包**

    ```sh
    tc qdisc add dev eth0 root netem loss 1%
    ```

    也可以设置丢包的成功率：**将eth0网卡的传输设置为随机丢掉1%的数据包，成功率为30%**

    ```sh
    tc qdisc add dev eth0 root netem loss 1% 30%
    ```

* 3、模拟包重复

    **将eth0网卡的传输设置为随机产生 1% 的重复数据包**   

    ```sh
    tc qdisc add dev eth0 root netem duplicate 1%
    ```

* 4、模拟包损坏

    **将 eth0 网卡的传输设置为随机产生 0.2% 的损坏的数据包(内核版本需在2.6.16以上)**

    ```sh
    tc qdisc add dev eth0 root netem corrupt 0.2% 
    ```

* 5、模拟包乱序

    **将eth0网卡的传输设置为: 有 25% 的数据包 (50%相关) 会被立即发送，其他的延迟 10 秒**

    ```sh
    tc qdisc change dev eth0 root netem delay 10ms reorder 25% 50%
    ```

    新版本中，如下命令也会在一定程度上打乱发包的次序:  

    ```sh
    tc qdisc add dev eth0 root netem delay 100ms 10ms
    ```

## dir,popd,pushd

`pushd` 和 `popd` 在linux中可以用来方便地在多个目录之间切换。

`pushd` 和 `popd` 是对一个目录栈进行操作，而 `dirs` 是显示目录栈的内容。

目录栈的栈顶永远存放的是当前目录。如果当前目录发生变化，那么目录栈的栈顶元素肯定也变了；反过来，如果栈顶元素发生变化，那么当前目录肯定也变了。

* dirs -- Display directory stack

    ```text
    -c 清空目录栈
    -l 列出栈中所有Index
    -p 列出栈中所有Index(每行一条Index)
    -v 列出栈中所有Index(每行一条Index, 且每行前带序号)

    +N 显示序号为N的Index
    -N 显示序号为N的Index(倒序)
    ```

* pushd -- Add directories to stack.

    ```text
    不带任何参数时 切换回上一次的目录(即序号为1的Index)
    -n 仅操作堆栈, 不切换当前目录(即栈顶不发生改变)

    +N  栈顶切换到序号为N的Index, 如果该Index的目录不存在, 该Index会被修改成当前目录
    -N  栈顶切换到序号为N的Index(倒序)
    DIR 目录切换到DIR, 并将DIR添加到栈顶
    ```

* popd -- Remove directories from stack.

    ```text
    不带任何参数时 弹出栈顶Index
    -n 仅操作堆栈
        不带 +N/-N时，从栈底开始弹出Index
        +N 弹出序号为N的Index，如果Index为栈顶，则N+1
        -N 弹出序号为N的Index(倒序)，如果Index为栈顶，则N-1

    +N 弹出序号为N的Index
    -N 弹出序号为N的Index(倒序)
    ```


## 断开用户连接

```sh
fuser -k /dev/pty/1
# or
pkill -kill -t pts/2
```


## `i18n` 与 `l10n`

* `i18n`: Internationalization 国际化
* `l10n`: Locallization 本地化


## `/etc/fstab` 中的 `nofail` 参数

红帽 7 中新增 `nofail` 参数:

```sh
/dev/critical   /critical  xfs  defaults     1 2
/dev/optional   /optional  xfs  defaults,nofail  1 2
```

指定该参数时（如上示例），挂载到 `/optional` 的设备如果无法成功挂载，则不会导致引导失败。

## tmpfs, tmp.mount

临时文件系统，将 **内存/swap** 中的空间 “划分” 出来挂载到目录，可以存放需要高速读写的文件。

* 由于是存放在内存或swap中，数据只是暂存，系统重启后会丢失。
* 默认大小为内存的 1/2
* 如何挂载 tmpfs ?

    ```sh
    mount -t tmpfs -o size=100M tmpfs /mount_point # 新增一个
    mount -o remount,size=100M /mount_point        # 调整大小
    ```

* `/tmp` 可通过 systemd 自定的 `tmp.mount` 文件使用 tmpfs 挂载。（RHEL/CentOS 7) 


## tmpfile.d - 启动时重新创建文件和目录

> Only >= RHEL/CentOS 7

* 涉及目录：

    ```text
    /etc/tmpfiles.d/*.conf 
    /run/tmpfiles.d/*.conf
    /usr/tmpfiles.d/*.conf
    ```

* 优先级 - 按上面顺序从高到低，也就是 `/etc/tmpfiles.d/` 中的文件会覆盖 `/run/tmpfiles.d/`, `/usr/tmpfiles.d/` 中的**同名文件**

* 所有的配置文件名会按字典顺序排序（三个目录一起排序，不区分），如果多个配置文件中出现了相同路径的配置行（the same path），排序最靠前的配置文件中的配置行会应用成功，而其他配置文件中的会在日志记录错误。

* 如果两个配置行的路径之间存在 “父子” 关系，会先执行 “父”。（ 如会先配置 /A，然后配置 /A/B）

* 如果想禁用某个软件/服务商提供的配置文件，可以在 `/etc/tmpfiles.d/` 创建一个同名的、指向 `/dev/null` 的软链接

* 配置格式：

    ```text
    #Type   Path        Mode    UID     GID     Age Argument
        d   /run/user   0755    root    root    10d -
        L   /tmp/foobar -       -       -       -   /dev/null
    ```

    * Type

        | Type |                                              |
        | ---- | -------------------------------------------- |
        | f    | <u>f</u>ile      (if not exist)              |
        | F    | <u>f</u>ile      (always)                    |
        | w    | <u>w</u>rite     (if exist)                  |
        | d    | <u>d</u>irectory (if not exist)              |
        | D    | <u>d</u>irectory (always)                    |
        | e    | cl<u>e</u>an                                 |
        | v    | sub<u>v</u>olume(brtfs), or same as <u>d</u> |
        | p,p+ | <u>p</u>ipe (FIFO)                           |
        | L,L+ | sym<u>l</u>ink                               |
        | c,c+ | <u>c</u>haracter device                      |
        | b,b+ | <u>b</u>lock device                          |
        | C    | <u>c</u>opy                                  |
        | x    | (for clean) e<u>x</u>clude                   |
        | X    | (for clean) e<u>x</u>clude, only itself      |
        | r    | <u>r</u>emove                                |
        | R    | <u>r</u>ecursively remove                    |
        | z    | access mode, group, user                     |
        | Z    | access mode, group, user (recursively)       |
        | t    | ex<u>t</u>ended attributes                   |
        | T    | recursively ex<u>t</u>ended attributes       |
        | a,a+ | <u>a</u>cl                                   |
        | A,A+ | recursively <u>a</u>cl                       |

        > 使用 `!` 标记的操作（如 `r!`)，表示只在系统启动过程中执行，后续不再运行。

    * Path

        路径，支持简单的参数扩展：

        | Specifier | Meaning        | Details                   |
        | --------- | -------------- | ------------------------- |
        | `%m`      | Machine ID     | `/etc/machine`            |
        | `%b`      | Boot ID        | `journalctl --list-boots` |
        | `%H`      | Host name      |                           |
        | `%v`      | Kernel release | `uname -r`                |
        | `%%`      | Escaped '%'    |                           |

    * Mode

        * Default: `0755` for directory, `0644` for other object

        * 前缀 `~` - 表示将此字段当作权限掩码 (umask) 来设置

        * This parameter is **ignored** for <u>x</u>, <u>r</u>, <u>R</u>, <u>L</u>, <u>t</u> and <u>a</u> lines.

    * UID, GID

        * 设置 user 和 group，可以用用户名/组名，也可以指定 UID 和 GID。

        * This parameter is **ignored** for <u>x</u>, <u>r</u>, <u>R</u>, <u>L</u>, <u>t</u> and <u>a</u> lines.

    * Age

        用于判断目录、文件是否 “过期”，并将过期文件清理。

        * 单位: s(Default), m/min, h, d, w, ms, us

        * `0` - 不设置过期条件，全部清理； `-` - 不进行任何清理，保留所有文件

        * 如果数值以 `~` 开头 - 表示保留指定目录直属的文件与子目录，仅清理直属子目录下的内容

        * This parameter is **only** for <u>d</u>, <u>D</u>, <u>e</u> and <u>x</u> lines.

    * Aguement
        
        * for `L` - the destination path of symlink
        * for `c`, `b` - the major/minor of the device node (e.g. `1:3`)
        * for `f`, `F`, `w` - a short string that is written to the file
        * for `c` - source file or directory
        * for `t` - extended attributes to be set 
        * for `a` - ACL attributes to be set

* 自动清理

    对应服务unit: 
    
    * `/usr/lib/systemd/system/systemd-tmpfiles-clean.timer`

        ```sh
        [Timer]
        OnBootSec=15min     # 开机后15min执行一次
        OnUnitActiveSec=1d  # 每天执行一次（距离上次执行该服务1天后）
        ```

    * `/usr/lib/systemd/system/systemd-tmpfiles-clean.service`

        ```sh
        [Service]
        Type=oneshot
        ExecStart=/usr/bin/systemd-tmpfiles --clean
        IOSchedulingClass=idle
        ```


* 示例

    ```sh
    # 1. 按照指定的User/Group、权限、清理周期创建目录
    #    开机后自动创建 /run/screens，并设置 /run/screens 中超过10天的内容将被清理
    d /run/screens  1777 root screen 10d

    # 2. 在启动时清空目录
    e! /var/cache/krb5rcache - - - 0
    ```

## 修改 `/tmp` 自动清理时间

* 7.x

    7.x 通过 systemd 服务指定的规则清理对应目录, 相关 systemd 服务为:

    ```sh
    # systemd-tmpfiles-setup.service         创建
    # systemd-tmpfiles-setup-dev.service     创建
    # systemd-tmpfiles-clean.service         清理
    # systemd-tmpfiles-clean.timer           定时器

    systemctl status systemd-tmpfiles-clean.timer
    ```

    修改: 

    ```sh
    ~] vi /usr/lib/tmpfiles.d/tmp.conf 

    # Clear tmp directories separately, to make them easier to override
    v /tmp 1777 root root 10d          # <=修改此处即可
    v /var/tmp 1777 root root 30d      # <=修改此处即可

    # Exclude namespace mountpoints created with PrivateTmp=yes
    x /tmp/systemd-private-%b-*
    X /tmp/systemd-private-%b-*/tmp
    x /var/tmp/systemd-private-%b-*
    X /var/tmp/systemd-private-%b-*/tmp
    ```

* 6.x

    6.x 是通过定时任务定期清理:

    ```sh
    ~] cat /etc/cron.daily/tmpwatch  # 6.9 最小化安装没有

    #! /bin/sh
    flags=-umc
    /usr/sbin/tmpwatch "$flags" -x /tmp/.X11-unix -x /tmp/.XIM-unix \
        -x /tmp/.font-unix -x /tmp/.ICE-unix -x /tmp/.Test-unix \
        -X '/tmp/hsperfdata_*' 10d /tmp
    /usr/sbin/tmpwatch "$flags" 30d /var/tmp
    for d in /var/{cache/man,catman}/{cat?,X11R6/cat?,local/cat?}; do
        if [ -d "$d" ]; then
        /usr/sbin/tmpwatch "$flags" -f 30d "$d"
        fi
    done
    ```


## systemd 中的 limit


* `/etc/security/limits.conf` 是 `pam_limits.so` 的配置文件，可以通过 `man limits.conf` 看到:

    ```text
    LIMITS.CONF(5)                 Linux-PAM Manual                 LIMITS.CONF(5)

    NAME
        limits.conf - configuration file for the pam_limits module

    DESCRIPTION
        The pam_limits.so module applies ulimit limits, nice priority
        and number of simultaneous login sessions limit to user login
        sessions. This description of the configuration file syntax
        applies to the /etc/security/limits.conf file and *.conf files
        in the /etc/security/limits.d directory.
    ```

* PAM 全称为插入式验证模块（Pluggable Authentication Module，PAM），主要目的是为Linux下不同依赖用户体系的应用程序提供统一身份认证和用户资料读写API。

    `man 8 pam`:

    ```text
    PAM(8)                         Linux-PAM Manual                         PAM(8)

    NAME
        PAM, pam - Pluggable Authentication Modules for Linux

    DESCRIPTION
        This manual is intended to offer a quick introduction to Linux-PAM.
        For more information the reader is directed to the Linux-PAM system
        administrators´ guide.

        Linux-PAM is a system of libraries that handle the authentication
        tasks of applications (services) on the system. The library provides
        a stable general interface (Application Programming Interface - API)
        that privilege granting programs (such as login(1) and su(1)) defer
        to to perform standard authentication tasks.
    ```

    描述中明确表示PAM既可以用于应用程序鉴权，也可以用于服务鉴权。这里的服务指的是以 init 进程为根进程的，被称作 SysV 的机制，也就是各发行版在使用 systemd 之前广泛使用的服务机制。

* 那么问题来了：对于 systemd，到底是否依旧沿用 PAM 模块实现资源限制呢？

    [Bug 754285 - Hint that /etc/security/limits.conf does not apply to systemd services](https://link.zhihu.com/?target=https%3A//bugzilla.redhat.com/show_bug.cgi%3Fid%3D754285) 帖子中提到了一模一样的问题。Systemd 的作者之一 Kay Sievers 当时给与了以下回复：

    ```text
    Systemd does not support global limits, the file is intentionally ignored. LimitNOFILE= in the service file can be set to specify the number of open file descriptors for a specific service.
    ```

    即 Systemd 设计的时候故意忽略了全局限制，转而在配置文件中配置对每个服务的资源限制，结合 /etc/security/limits.conf 文件开头的注释来看，果然如此：

    ```text
    # /etc/security/limits.conf
    #
    #This file sets the resource limits for the users logged in via PAM.
    #It does not affect resource limits of the system services.
    ...
    ```

* 如何在Systemd的配置文件中设置资源限制 ?

    `man systemd.exec` 查看映射关系: 

    ```text
    Table 1. Limit directives and their equivalent with ulimit
    ┌─────────────────┬───────────────────┬────────────────────────────┐
    │Directive        │ ulimit equivalent │ Unit                       │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitCPU=        │ ulimit -t         │ Seconds                    │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitFSIZE=      │ ulimit -f         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitDATA=       │ ulimit -d         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitSTACK=      │ ulimit -s         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitCORE=       │ ulimit -c         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitRSS=        │ ulimit -m         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitNOFILE=     │ ulimit -n         │ Number of File Descriptors │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitAS=         │ ulimit -v         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitNPROC=      │ ulimit -u         │ Number of Processes        │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitMEMLOCK=    │ ulimit -l         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitLOCKS=      │ ulimit -x         │ Number of Locks            │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitSIGPENDING= │ ulimit -i         │ Number of Queued Signals   │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitMSGQUEUE=   │ ulimit -q         │ Bytes                      │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitNICE=       │ ulimit -e         │ Nice Level                 │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitRTPRIO=     │ ulimit -r         │ Realtime Priority          │
    ├─────────────────┼───────────────────┼────────────────────────────┤
    │LimitRTTIME=     │ No equivalent     │ Microseconds               │
    └─────────────────┴───────────────────┴────────────────────────────┘
    ```

    综上，解决办法有以下两种：

    * 直接修改 systemd 服务对应的配置文件，在 `[service]` 下添加对应资源限制
    
    * 或者修改 systemd 默认配置，通过 `/etc/systemd/system.conf` 和 `/etc/systemd/user.conf` 配置 (帮助文档 `man systemd-system.conf`)

## "11 minute mode"

Now, when the system clock is synchronized by the `Network Time Protocol` (NTP) or `Precision Time Protocol` (PTP), the kernel automatically synchronizes the hardware clock to the system clock every 11 minutes.

## PROMPT_COMMAND

If set, the value is executed as a command prior to issuing each primary prompt.

```sh
#!/bin/bash
# 使用 root 用户执行

# 创建日志文件
touch /var/log/cmd_audit.log

# 修改权限
chown root:root /var/log/cmd_audit.log
chmod 622 /var/log/cmd_audit.log

# 配置 /etc/profile
echo "export HISTORY_FILE=/var/log/cmd_audit.log" >> /etc/profile
echo -E """export PROMPT_COMMAND='{ thisHistID=\`history 1|awk \"{print \\\\\$1}\"\`; lastcommand=\`history 1|awk \"{\\\\\$1=\\\"\\\" ;print}\"\`;user=\`id -un\`; pwd=\`pwd\`;who_info=(\`who -u am i\`); login_date=\${who_info[3]}; login_time=\${who_info[4]}; login_pid=\${who_info[5]}; login_ip=\${who_info[6]}; if [ \"\${thisHistID}x\" != \"\${lastHistID}x\" ];then echo -E [\$(date \"+%Y/%m/%d %H:%M:%S\")] \${login_ip} [sshpid:\${login_pid}] [\${user}@\${pwd}]   \$lastcommand ; lastHistID=\$thisHistID;fi; } >> /var/log/cmd_audit.log'""" >> /etc/profile
```

## Cron 一个示例

* 每个月第一周的周日重启

    ```sh
    0 2 * * 7 [ $(date +\%d  -d "-0 days") -ge 1 -a $(date +\%d -d "-0 days") -le 7] && sudo reboot
    ```

## SSH 跳转服务器 一个示例

本示例实现本地可以访问内网KVM HOST上运行的所有KVM guestos机器。

```text
本机(10.8.148.x)  -> KVM HOST(10.2.100.x) -> KVM GUESTOS(192.168.161.*)
```

1. 本地不能直接访问kvm host的22端口，但是可以可以访问其他端口（如8822）。为了能够正常访问，有两种选择：

    * 将ssh端口修改为8822 - 因为内网还有其他机器需要ssh连接到 kvm host, 该方法不合适
    * nginx 4层转发 :

        ```conf
        stream {
            upstream ssh {
                server 127.0.0.1:22;
            }
            server {
                listen 10.2.100.:8822;
                proxy_pass ssh;
            }
        }
        ```

        在kvm host本地安装nginx进行转发。

2. 本地编辑 `~/.ssh/config`

    ```conf
    Host KVM_HOST Jump 10.2.100.x
        Hostname 10.2.100.x
        Port 8822
        User root
        GSSAPIAuthentication no
        BindAddress 10.8.148.x
        IdentityFile ~/.ssh/id_rsa

    Match User root, Exec "echo %h | grep 192.168.16[12]."
        GSSAPIAuthentication no
        ProxyCommand ssh Jump -W %h:%p
    ```

    注：
    
    * `GSSAPIAuthentication no` 忽略 GSSAPIAuthentication 
    * `BindAddress` 用于指定本地IP，如果只有一个IP可以省略
    * `ProxyCommand ssh Jump -W %h:%p` 跳转服务的关键，`%h`为主机IP，`%p`为端口

3. 本地可以用脚本管理

   * jump_server.sh

        ```sh
        #! /bin/bash

        SERVER_LIST="/home/chenwen1/server_list"
        [ ! -f "$SERVER_LIST" ] && exit 1

        if [ -z "$1" ]; then
            echo 'Happy Every Day!!!'

            cat $SERVER_LIST
            echo -n 'Please choose one to connect: '
            read c 
        else
            c="$1"
        fi

        [ -z "$c" ] && exit 2
        IP=$(grep "^\[$c\]" $SERVER_LIST  | awk '{print $3}')
        ssh root@$IP
        ```
    
    * server_list

        ```text
        ID      Hostname        IP
        ----------------------------------------
        [1]     centos          192.168.161.2
        [2]     rhel76-qnetd    192.168.161.11
        [3]     rhel76-node01   192.168.161.12
        [4]     rhel76-node01   192.168.161.13
        ```

    可以直接使用，也可以写成一个alias，后续使用比较方便：

    ```sh
    alias JumpServer='bash /home/chenwen1/jump_server.sh'
    ```

    使用示例：

    ```sh
    chenwen1@abcdef:~$ JumpServer 
    Happy Every Day!!!
    ID      Hostname        IP
    ----------------------------------------
    [1]     centos          192.168.161.2
    [2]     rhel76-qnetd    192.168.161.11
    [3]     rhel76-node01   192.168.161.12
    [4]     rhel76-node01   192.168.161.13
    Please choose one to connect: 2
    Last login: Wed May 24 09:37:40 2023 from gateway
    [root@rhel76-qnetd ~]# exit
    
    ```

## Curl 调用接口 一个示例

java登录代码：

```java
...
Request request=new Request(Method.POST, url);
Series<Header> extraHeaders = new Series(Header.class);
extraHeaders.add("X-Auth-User", userName)
extraHeaders.add("X-Auth-Key", SHA256Utils.encrypt(password));
request.getAttributes().put("org.restet.httpheaders", extraHeaders);
this.setDefaultHttpHeader(request, (String) null);
client.getContext().getParameters().add("socketConnectTimeoutMs", String.value0f(20000));
client.getContext().getParameters().add("readTimeout", String.value0f(20000));
String method ="POST";
String body ="";
String reqTime=Logutil.getSysTime();
```

转换成curl调用：

1. url：通过日志捞出来为 `https://172.24.165.62:57442/service/session`

2. header：有两个，分别为 `X-Auth-User` 和 `X-Auth-Key`，其中`X-Auth-Key`需要使用sha256加密

3. sha256加密：linux系统可借助 `openssl` 对字符串进行加密

    ```sh
    # Usage: openssl dgst [-digest] ...
    
    $ openssl list --digest-commands
    blake2b512        blake2s256        gost              md4               
    md5               rmd160            sha1              sha224            
    sha256            sha3-224          sha3-256          sha3-384          
    sha3-512          sha384            sha512            sha512-224        
    sha512-256        shake128          shake256          sm3 

    # 对字符串 'password' 加密：
    $ echo -n 'password' | openssl dgst -sha256 
    (stdin)= 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8
    ```

4. curl完整调用为：

    ```sh
    curl -X POST -k https://172.24.165.62:57442/service/session -H 'X-Auth-User: feizhiyun' -H 'X-Auth-Key: 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8'
    ```